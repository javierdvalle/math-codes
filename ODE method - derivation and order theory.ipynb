{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema de Valor Inicial\n",
    "\n",
    "Dado un PVI: $\n",
    "\\begin{cases}\n",
    "y'(t) = f(t, y(t)) \\\\\n",
    "y(t_0) = y_0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Nuestro objetivo es encontrar la función $y(t)$, es decir, integrar f(t,y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivar f(t,y)\n",
    "\n",
    "En los PVI de las EDO es importante el desarrollo de Taylor de la funcion compuesta $f(t, y(t))$. Y para ello es necesario calcular sus derivadas.\n",
    "\n",
    "Al calcular las derivadas para el desarrollo de Taylor, la regla de la cadena se complica bastante:\n",
    "\n",
    ">$\n",
    "f'(t, y(t)) = \\frac{d}{d t}f(t, y(t)) = \\frac{\\partial f(t, y)}{\\partial t} + \\frac{\\partial f(t, y)}{\\partial y} \\frac{\\partial y(t)}{\\partial t} = f_t(t, y) + f_y(t, y) f(t,y) = f_t + f_y f \\\\\n",
    "f''(t, y(t)) = \\frac{d}{d t} [\\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial t}] = (\\frac{\\partial^2 f}{\\partial t^2} + \\frac{\\partial^2 f}{\\partial ty } \\frac{\\partial y}{\\partial t}) + [ (\\frac{\\partial^2 f}{\\partial y \\partial t} + \\frac{\\partial^2 f}{\\partial y^2} \\frac{\\partial y}{\\partial t}) \\frac{\\partial y}{\\partial t} + \\frac{\\partial f}{\\partial y} \\frac{\\partial^2 y}{\\partial t^2}] = (f_{tt} + f_{ty} f) + [(f_{yt} + f_{yy}f)f + f_y f']\\\\\n",
    "f'''(t, y(t)) = \\dots \\\\\n",
    "$\n",
    "\n",
    "Hemos usado que $\\frac{\\partial y}{\\partial t} = f(t,y)$ y que podemos sustituir $f'$ por lo calculado en el primer paso.\n",
    "\n",
    "El número de términos crece rápidamente. Hay una teoría basada en árboles para analizar todos esos términos, desarrollada por J.C.Butcher. Más info: https://www.asiapacific-mathnews.com/07/0701/0001_0011.pdf\n",
    "\n",
    "Sin embargo, en la mayoria de métodos numéricos f(t,y) es relativamente simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taylor expansion & method derivation.\n",
    "\n",
    "\n",
    "La idea para construir un método es tratar de parecerse lo máximo posible a la solución exacta $y(t)$, o más bien a su desarrollo de Taylor en $t=t_0$:\n",
    "\n",
    ">$\n",
    "y(t) \\overset{\\text{Taylor}}{=} y(t_0) + \\frac{y'(t_0)}{1!}(t-t_0) + \\frac{y''(t_0)}{2!}(t-t_0)^2 + ... + \\frac{y^{(n)}(t_0)}{n!}(t-t_0)^n + O((t-t_0)^{n+1})\n",
    "$\n",
    "\n",
    "Es importante señalar que en esa ecuación ambos lados son EXACTAMENTE IGUALES gracias a que la parte derecha incluye el término de error $O(h^{n+1})$.\n",
    "\n",
    "Si queremos usar ese desarrollo de taylor como método numérico, podemos \"avanzar un paso\" evaluando en $t_1$: (usamos que $h=t_1-t_0$)\n",
    "\n",
    ">$\n",
    "y(t_1) \\overset{\\text{Taylor}}{=} y(t_0) + \\frac{y'(t_0)}{1!}(h) + \\frac{y''(t_0)}{2!}(h)^2 + ... + \\frac{y^{(n)}(t_0)}{n!}(h)^n + O(h^{n+1})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local truncation error. Order.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Truncation_error_(numerical_integration)\n",
    "\n",
    "\n",
    "> **Error local (truncation error)**\n",
    "> $$\\tau = exacta - numérica = y(t_0+h) - \\hat{y}(t_0+h) \\\\ \\quad = E(h) - N(h)$$\n",
    "> \n",
    "> donde $E(h)$ y $N(h)$ se suelen sustituir por su desarrollo de Taylor alrededor de $h=0$\n",
    ">\n",
    "> Un método tiene **orden n** si tiene error local $O(h^{n+1})$ es decir, si $E(h)$ y $N(h)$ son iguales hasta el término $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo sencillo cálculo error local: Método de Euler\n",
    "\n",
    "Cambiamos de notación:\n",
    "$ \n",
    "y_{n+1} = y_n + hf(t_n, y_n) \\quad \\rightarrow \\quad \\hat{y}(t_0+h) = y(t_0) + hf(t_0, y(t_0)) \\\\ \n",
    "$\n",
    "\n",
    "Sustituimos el método en la ecuación del error local, y sustituimos la solución exacta por su desarrollo de Taylor, y tambien usamos $y'(t) = f(t,y)$:\n",
    "\n",
    "$\n",
    "\\tau = y(t_0+h) - \\hat{y}(t_0+h) = \\\\\n",
    "\\quad = [y(t_0+h)] - [y(t_0) + hf(t_0,y(t_0))] = \\\\\n",
    "\\quad = [y(t_0) + \\frac{y'(t_0)}{1!}(t_0+h-t_0) + O(h^2)] - [y(t_0) + hf(t_0,y(t_0))] = \\\\\n",
    "\\quad = y'(t_0)h + O(h^2) - hf(t_0,y(t_0)) = \\\\\n",
    "\\quad = hf(t_0,y(t_0)) + O(h^2) - hf(t_0,y(t_0)) \\quad = \\quad O(h^2)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivar un método: intuición\n",
    "\n",
    "De hecho, el desarrollo de Taylor completo sería un método numérico perfecto si usásemos infinitos términos. Pero tenemos dos problemas: el primero, que obviamente no podemos usar infinitos términos; y el segundo, que necesitaríamos calcular las derivadas analíticamente (\"a mano\").\n",
    "\n",
    "El primer problema se soluciona de la forma obvia: nos quedamos con unos cuantos términos, y el resto lo metemos en el término de error (¡precisamente este término de error es el que llamamos error local!). El segundo problema, el de calcular las derivadas a mano, es donde radica la magia de crear un método numérico.\n",
    "\n",
    "La primera derivada si la tenemos, ya que por definición $y'(t) = f(t,y)$, y conocemos $f$. Las demás se aproximan componiendo (\"nesting\") esa función $f$.\n",
    "\n",
    "En la realidad solemos partir ya de una fórmula general para el método numérico, con una serie de parámetros que queremos ajustar para conseguir que tenga orden máximo. Lo que hacemos es utilizar la fórmula del error local y desarrollar $E(h)$ y $N(h)$ con Taylor, igualar los términos, y encontrar los parámetros que hacen que los términos se cancelen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nota\n",
    "\n",
    ">$\n",
    "f(t_1, y_1)) = f(t_0, y_0) + h f'(t_0,y_0) + O(h^2)\n",
    "$\n",
    "\n",
    "\n",
    "NOTA: no tengo claro este punto, pero lo usan [aquí](http://www.ss.ncu.edu.tw/~lyu/lecture_files_en/lyu_NSSP_Notes/Lyu_NSSP_AppendixC.pdf) y [aquí](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#Derivation_of_the_Runge%E2%80%93Kutta_fourth-order_method). No tengo claro si esa aproximación estima $f(t_1, y(t_0))$ ó $f(t_1, y_1)$, que no son lo mismo ya que $y_1$ es obtenido numéricamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo derivar un método: Runge-Kutta orden 2\n",
    "\n",
    "http://web.mit.edu/10.001/Web/Course_Notes/Differential_Equations_Notes/node5.html\n",
    "\n",
    "La familia de [métodos de Runge-Kutta](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#Derivation_of_the_Runge%E2%80%93Kutta_fourth-order_method) de orden 2 es de forma: \n",
    "\n",
    ">$\n",
    "\\begin{cases}\n",
    "k_1 = f(t_n, y_n) \\\\\n",
    "k_2 = f(t_n + \\alpha h, y_n + \\beta h k_1)\n",
    "\\end{cases} \\\\\n",
    "y_{n+1} = y_n + h(b_1 k_1 + b_2 k_2)\n",
    "$\n",
    "\n",
    "Sustituyendo los $k_i$ explícitamente:\n",
    "\n",
    ">$\n",
    "y_{n+1} = y_n + h [b_1 f(t_n, y_n) + b_2 f(t_n + \\alpha h, y_n + \\beta h f(t_n, y_n))]\n",
    "$\n",
    "\n",
    "Calculamos el error local:\n",
    "\n",
    ">$\\tau = y(t_0+h) - \\hat{y}(t_0+h) = E(h) - N(h) \\\\\n",
    "\\begin{cases}\n",
    "E(h) = y(t_0+h) \\\\ \n",
    "N(h) = y_0 + h [b_1 f(t_0, y_0) + b_2 f(t_0 + \\alpha h, y_0 + \\beta h f(t_0, y_0))]\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Primero calculamos $N(h)$. Aquí el problema es $f(t_n + \\alpha h, y_n + \\beta h f(t_0, y_0))$ así que recurrimos al desarrollo de Taylor de $f(t(h), y(t(h)))$ respecto de h, alrededor de $h=0$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$\n",
    "f(t(h),y(h)) = f(t_0 + \\alpha h, y_0 + \\beta h f(t_0, y_0)) \\overset{\\text{Taylor}}{=} \\\\\n",
    "\\quad = f(y_0, t_0) + \\frac{\\frac{d f}{d h}\\vert_{h=0}}{1!}(h) + \\frac{\\frac{d^2 f}{d h^2}\\vert_{h=0}}{2!}(h)^2 + O(h^3) = \\\\ \n",
    "\\quad = f(t_0, y_0) + (f_t \\alpha + f_y \\beta f(t_0, y_0)) h + \\frac{1}{2}(f_{tt} \\alpha + f_{yy} \\beta^2 (f(t_0, y_0))^2)h^2 + O(h^3)\n",
    "$\n",
    "\n",
    "Las parciales se calculan:\n",
    "\n",
    ">$ \n",
    "\\begin{cases}\n",
    "    \\frac{d f}{d h} = \\frac{\\partial f}{\\partial t} \\frac{\\partial t}{\\partial h} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial h} = f_t \\alpha + f_y \\beta f \\\\\n",
    "    \\frac{d^2 f}{d h^2} = [(\\frac{\\partial^2 f}{\\partial t^2} \\frac{\\partial t}{\\partial h}) \\frac{\\partial t}{\\partial h} + \\frac{\\partial f}{\\partial t} \\frac{\\partial^2 t}{\\partial h^2}] + [(\\frac{\\partial^2 f}{\\partial y^2} \\frac{\\partial y}{\\partial h}) \\frac{\\partial y}{\\partial h} + \\frac{\\partial f}{\\partial y} \\frac{\\partial^2 y}{\\partial h^2}] = [f_{tt} \\alpha^2 + f_t \\cdot 0] + [f_{yy} \\cdot (\\beta f(t_0, y_0))^2 + f_y \\cdot 0] = f_{tt} \\alpha^2 + f_{yy} \\beta^2 f^2\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "En realidad solo necesitamos los dos primeros términos:\n",
    "\n",
    ">$\n",
    "f(t_0 + \\alpha h, y_0 + \\beta h f(t_0, y_0)) \\overset{\\text{Taylor}}{=} f(t_0, y_0) + (f_t \\alpha + f_y \\beta f(t_0, y_0)) h + O(h^2)\n",
    "$\n",
    "\n",
    "Por lo que $N(h)$ queda, agrupando por h:\n",
    "\n",
    ">$\n",
    "N(h) = y_0 + h \\big[b_1 f(t_0, y_0) + b_2 \\big(f(t_0, y_0) + h(f_t \\alpha + f_y \\beta f(t_0, y_0)) \\big)\\big] + O(h^3) = \\\\\n",
    "\\quad = y_0 + h (b_1 f(t_0, y_0) + b_2 f(t_0, y_0)) +  h^2 b_2(f_t \\alpha + f_y \\beta f(t_0, y_0)) + O(h^3)\n",
    "$\n",
    "\n",
    "Por otro lado, desarrollamos $E(h)$ con el desarrollo de Taylor:\n",
    "\n",
    ">$\n",
    "E(h) = y(t_0+h) = y(t_0) + \\frac{y'(t_0)}{1!}(h) + \\frac{y''(t_0)}{2!}(h)^2 + O(h^3) = y_0 + \\frac{f(t_0,y_0)}{1!}(h) + \\frac{f'(t_0, y_0)}{2!}(h)^2 + O(h^3) = y_0 + h f(t_0,y_0) + \\frac{h^2}{2} f'(t_0, y_0) + O(h^3) = y_0 + h f(t_0,y_0) + \\frac{h^2}{2} [f_t + f_y f(t_0, y_0)] + O(h^3)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para concluir, igualando términos en $E(h)$ y $N(h)$ tenemos:\n",
    "\n",
    ">$\n",
    "\\bullet \\quad y_0 = y_0 \\\\\n",
    "\\bullet \\quad h f(t_0, y_0) = h (b_1 f(t_0, y_0) + b_2 f(t_0, y_0)) \\rightarrow b_1 + b_2 = 1 \\\\\n",
    "\\bullet \\quad \\frac{h^2}{2} [f_t + f_y f(t_0, y_0)] = h^2 b_2(f_t \\alpha + f_y \\beta f(t_0, y_0)) \\rightarrow b_2 \\alpha = b_2 \\beta = \\frac{1}{2}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay infinitas posibilidades de esos parámetros. En concreto, si elegimos $\\alpha = \\beta = 1$ y $b_1 = b_2 = \\frac{1}{2}$ nos queda el [**método de Heun explícito**](https://en.wikipedia.org/wiki/Heun%27s_method) (también llamado **método del trapecio explícito**):\n",
    "\n",
    ">$\n",
    "y_{n+1} = y_n + \\frac{h}{2}[f(t_n, y_n) + f(t_{n+1},y_n+hf(t_n, y_n))],\n",
    "$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
